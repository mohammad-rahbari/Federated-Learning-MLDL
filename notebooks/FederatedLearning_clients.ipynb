{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammad-rahbari/Federated-Learning-MLDL/blob/master/notebooks/FederatedLearning_clients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBf85ZRGj6HO",
        "outputId": "20794806-9b19-4dc2-abcc-cb259a62484f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu0Iz0LDlRDK"
      },
      "source": [
        "# Importing DINO and installing its dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IckKkLcG0zeA",
        "outputId": "193d6299-9672-46b0-9522-ad392c1d90ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dino'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Total 175 (delta 0), reused 0 (delta 0), pack-reused 175 (from 1)\u001b[K\n",
            "Receiving objects: 100% (175/175), 24.47 MiB | 41.00 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Clon the DINO ripo\n",
        "!git clone https://github.com/facebookresearch/dino.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dY0yvkTfiNV3",
        "outputId": "03f6977b-7ac0-47af-c0f1-32fa5da48160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dino\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "# @title Installing required dependencies regarding DINO\n",
        "%cd dino\n",
        "!pip install -r requirements.txt\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dDodXJD_lPCd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split,DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hv0ik3jZ-8"
      },
      "source": [
        "\n",
        "# preprocessing the CIFAR-100 dataset\n",
        "\n",
        "feature size in CIFAR is 32x32 but DINO requires 224x224 in the input layer.\n",
        "\n",
        "In first step we upscale the dataset and then we add randomization to it\n",
        "\n",
        "In last step of transformation we normalize data usind mean value and standard division of ImageNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_4ydT67FmAQR"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNgGMkqaqX-G",
        "outputId": "30778ffe-a162-4ae4-cb33-274027836576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 46.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of combined dataset: 50000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR100\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "full_train = train_dataset\n",
        "\n",
        "# Verify the length of the new dataset\n",
        "print(f\"Length of combined dataset: {len(full_train)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dNQ67lu7cYNj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Imports\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4oXrqMHwAit"
      },
      "source": [
        "# Set Hyperparameters regarding the data spliting here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VJfqNlD_ORqk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title set the parameters here!!\n",
        "\n",
        "\n",
        "\n",
        "number_of_clients = None\n",
        "train_frac = 0.8 #@param\n",
        "val_frac = 0.2 #@param\n",
        "batch_size = 32 #@param{type:\"integer\"}\n",
        "is_seed_fixed = True #@param{type:\"boolean\"}\n",
        "seed = 42 #@param{type:\"integer\"}\n",
        "\n",
        "def set_seed(seed=42, is_seed_fixed=True):\n",
        "  if not is_seed_fixed:\n",
        "    return\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown </br> <h5>Indicate the number of clients that contribute in training:</h5>\n",
        "n_clients = 100 #@param{type:\"integer\"}\n",
        "\n",
        "#@markdown </br></br> <b>splitting hyperparameters</b>\n",
        "\n",
        "spliting_method = \"i.i.d. sharing\" #@param[\"i.i.d. sharing\",\"non-i.i.d. sharing\"]\n",
        "backbone = \"dino_vits16\" #@param[\"dino_resnet50\", \"dino_vits16\", \"dino_xcit_small_12_p16\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W4gR7IvnxqL6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Set the parameters here only if <b>non-i.i.d. sharing</b> method had been selected!!\n",
        "#@markdown Nc is the number of classes that each subset can contain\n",
        "if spliting_method == \"non-i.i.d. sharing\":\n",
        "  Nc = 25 #@param{type:\"integer\"}\n",
        "\n",
        "  # are_classes_overlaping = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown <h3>If we consider the Number of classes M and nummber of client K then:</h3>\n",
        "#@markdown <ul>\n",
        "#@markdown   <li>Nc should be:\n",
        "#@markdown     <ul>\n",
        "#@markdown       <li>\n",
        "#@markdown         Greater than or equal to <b>\\\\(\\frac{M}{K}\\\\)</b>\n",
        "#@markdown       </li>\n",
        "#@markdown       <li>\n",
        "#@markdown         Less than or equal to K </b>\n",
        "#@markdown       </li>\n",
        "#@markdown     </ul>\n",
        "#@markdown   </li>\n",
        "#@markdown   <li>\n",
        "#@markdown   Muximum number of clients means all classes contribute in every client\n",
        "#@markdown   </li>\n",
        "\n",
        "#@markdown </ul>\n",
        "\n",
        "\n",
        "#@markdown </br></br><h3>Combination of classes are randomly selected which suits definition of federated learning especially Cross-device federated learning</h3>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtH-qLIrAeh"
      },
      "source": [
        "# Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmFcWGtXrLS1",
        "outputId": "01cc31c5-3e93-4660-e837-f293d3bbbd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Size of subset:  [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n"
          ]
        }
      ],
      "source": [
        "# @title data splitting\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "total_size = len(full_train)\n",
        "train_size = int(train_frac * total_size)\n",
        "val_size   = total_size - train_size\n",
        "\n",
        "train_set, val_set = random_split(full_train, [train_size, val_size], generator=generator)\n",
        "train_indices = torch.tensor(train_set.indices)\n",
        "val_indices = torch.tensor(val_set.indices)\n",
        "\n",
        "train_set = Subset(train_set.dataset, train_indices)\n",
        "val_set = Subset(val_set.dataset, val_set.indices)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "val_loader  =  DataLoader(val_set, batch_size=len(val_set), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Train dataset size: {len(train_set)}\")\n",
        "print(f\"Validation dataset size: {len(val_set)}\")\n",
        "\n",
        "lenghts = [train_size//n_clients] * n_clients\n",
        "\n",
        "for i in range(train_size % n_clients):\n",
        "  lenghts[i] += 1\n",
        "print(\"Size of subset: \", lenghts)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "n86IvSfkp9Zv",
        "outputId": "ea7e5b27-46b2-48d9-bc66-036082fea925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0 has 389 samples\n",
            "Client 1 has 391 samples\n",
            "Client 2 has 388 samples\n",
            "Client 3 has 398 samples\n",
            "Client 4 has 391 samples\n",
            "Client 5 has 405 samples\n",
            "Client 6 has 397 samples\n",
            "Client 7 has 403 samples\n",
            "Client 8 has 401 samples\n",
            "Client 9 has 406 samples\n",
            "Client 10 has 409 samples\n",
            "Client 11 has 406 samples\n",
            "Client 12 has 396 samples\n",
            "Client 13 has 403 samples\n",
            "Client 14 has 400 samples\n",
            "Client 15 has 396 samples\n",
            "Client 16 has 402 samples\n",
            "Client 17 has 411 samples\n",
            "Client 18 has 394 samples\n",
            "Client 19 has 406 samples\n",
            "Client 20 has 396 samples\n",
            "Client 21 has 393 samples\n",
            "Client 22 has 396 samples\n",
            "Client 23 has 389 samples\n",
            "Client 24 has 391 samples\n",
            "Client 25 has 389 samples\n",
            "Client 26 has 388 samples\n",
            "Client 27 has 400 samples\n",
            "Client 28 has 404 samples\n",
            "Client 29 has 396 samples\n",
            "Client 30 has 408 samples\n",
            "Client 31 has 416 samples\n",
            "Client 32 has 388 samples\n",
            "Client 33 has 397 samples\n",
            "Client 34 has 402 samples\n",
            "Client 35 has 405 samples\n",
            "Client 36 has 402 samples\n",
            "Client 37 has 398 samples\n",
            "Client 38 has 410 samples\n",
            "Client 39 has 398 samples\n",
            "Client 40 has 406 samples\n",
            "Client 41 has 404 samples\n",
            "Client 42 has 400 samples\n",
            "Client 43 has 392 samples\n",
            "Client 44 has 398 samples\n",
            "Client 45 has 395 samples\n",
            "Client 46 has 399 samples\n",
            "Client 47 has 406 samples\n",
            "Client 48 has 406 samples\n",
            "Client 49 has 398 samples\n",
            "Client 50 has 398 samples\n",
            "Client 51 has 398 samples\n",
            "Client 52 has 400 samples\n",
            "Client 53 has 404 samples\n",
            "Client 54 has 401 samples\n",
            "Client 55 has 397 samples\n",
            "Client 56 has 407 samples\n",
            "Client 57 has 403 samples\n",
            "Client 58 has 411 samples\n",
            "Client 59 has 400 samples\n",
            "Client 60 has 399 samples\n",
            "Client 61 has 393 samples\n",
            "Client 62 has 400 samples\n",
            "Client 63 has 399 samples\n",
            "Client 64 has 403 samples\n",
            "Client 65 has 394 samples\n",
            "Client 66 has 386 samples\n",
            "Client 67 has 402 samples\n",
            "Client 68 has 404 samples\n",
            "Client 69 has 406 samples\n",
            "Client 70 has 409 samples\n",
            "Client 71 has 398 samples\n",
            "Client 72 has 405 samples\n",
            "Client 73 has 404 samples\n",
            "Client 74 has 394 samples\n",
            "Client 75 has 398 samples\n",
            "Client 76 has 400 samples\n",
            "Client 77 has 399 samples\n",
            "Client 78 has 400 samples\n",
            "Client 79 has 390 samples\n",
            "Client 80 has 395 samples\n",
            "Client 81 has 407 samples\n",
            "Client 82 has 410 samples\n",
            "Client 83 has 403 samples\n",
            "Client 84 has 404 samples\n",
            "Client 85 has 397 samples\n",
            "Client 86 has 409 samples\n",
            "Client 87 has 402 samples\n",
            "Client 88 has 399 samples\n",
            "Client 89 has 405 samples\n",
            "Client 90 has 398 samples\n",
            "Client 91 has 395 samples\n",
            "Client 92 has 407 samples\n",
            "Client 93 has 391 samples\n",
            "Client 94 has 403 samples\n",
            "Client 95 has 400 samples\n",
            "Client 96 has 399 samples\n",
            "Client 97 has 404 samples\n",
            "Client 98 has 409 samples\n",
            "Client 99 has 399 samples\n",
            "40000 40000\n"
          ]
        }
      ],
      "source": [
        "# @title i.i.d sharing - split data dased on number of clients and with respect of label proportionality\n",
        "set_seed(seed,is_seed_fixed)\n",
        "def iid_sharing(dataset, n_clients):\n",
        "\n",
        "  full_train_indices = dataset.indices\n",
        "  full_train_labels = torch.from_numpy(np.array(dataset.dataset.targets)[full_train_indices]) #collects labels from all dataset\n",
        "  unique_lables = torch.unique(full_train_labels) #Removes dupilication and generates a uniuqe list of labels (classes)\n",
        "  classes_indices = {}\n",
        "\n",
        "\n",
        "  for i in unique_lables:\n",
        "    classes_indices[i] = (full_train_labels == i).nonzero(as_tuple=True)[0] #Collects and save Indices in an array based on classes\n",
        "\n",
        "  for i in classes_indices.keys():\n",
        "    classes_indices[i] = classes_indices[i][torch.randperm(classes_indices[i].shape[0])] #suffels the indices\n",
        "\n",
        "\n",
        "  client_indices = {}\n",
        "\n",
        "\n",
        "\n",
        "  for client in range(n_clients):\n",
        "    if not client_indices.get(client):\n",
        "      client_indices[client] = torch.empty(0, dtype=torch.long)\n",
        "\n",
        "\n",
        "  for label in classes_indices.keys():\n",
        "    class_sample_size = len(classes_indices[label]) // n_clients\n",
        "    for k in range(n_clients):\n",
        "\n",
        "      client_indices[k] = torch.cat((client_indices[k], classes_indices[label][class_sample_size * k : class_sample_size * (k + 1)]), dim=0)\n",
        "\n",
        "  for label in classes_indices.keys():\n",
        "    remainder  = len(classes_indices[label]) % n_clients\n",
        "\n",
        "    for r in range(1,remainder+1):\n",
        "      random_client = random.choice(list(client_indices.keys()))\n",
        "      client_indices[random_client] = torch.cat((client_indices[random_client], classes_indices[label][-r].unsqueeze(0)), dim=0)\n",
        "\n",
        "  full_train_indices_t = torch.as_tensor(full_train_indices, dtype=torch.long)\n",
        "  client_data = {\n",
        "      client_id: Subset(\n",
        "          dataset.dataset,\n",
        "          full_train_indices_t[indices][torch.randperm(len(indices))].tolist()\n",
        "      )\n",
        "      for client_id, indices in client_indices.items()\n",
        "  }\n",
        "\n",
        "\n",
        "  #split actual dataset to multiple subset for clients\n",
        "  # client_data={\n",
        "  #     client_id: Subset(dataset.dataset,indices[torch.randperm(len(indices))])\n",
        "  #     for client_id, indices in client_indices.items()\n",
        "  # }\n",
        "  return client_data\n",
        "\n",
        "indices_check = []\n",
        "client_data = iid_sharing(train_set, n_clients)\n",
        "s = 0\n",
        "for client_id in client_data.keys():\n",
        "  indices_check = indices_check + list(client_data[client_id].indices)\n",
        "  s+= len(client_data[client_id])\n",
        "  print(f\"Client {client_id} has {len(client_data[client_id])} samples\")\n",
        "print(s, len(train_set))\n",
        "del indices_check,client_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UHYGEz82-ZJs"
      },
      "outputs": [],
      "source": [
        "# @title Non i.i.d sharing\n",
        "\n",
        "\n",
        "# @title i.i.d sharing - split data dased on number of clients and with respect of label proportionality\n",
        "set_seed(seed,is_seed_fixed)\n",
        "def noniid_sharing(dataset,Nc , n_clients):\n",
        "\n",
        "  full_train_indices = dataset.indices\n",
        "  full_train_labels = torch.tensor(dataset.dataset.targets)[full_train_indices] #collects labels from all dataset\n",
        "  unique_lables = torch.unique(full_train_labels) #Removes dupilication and generates a uniuqe list of labels (classes)\n",
        "\n",
        "  classes_indices = {}\n",
        "  classes_size = torch.zeros(unique_lables.size()[0])\n",
        "\n",
        "  class_combs = get_class_combinations(unique_lables, Nc, n_clients)\n",
        "\n",
        "  classes_num_partition = torch.zeros(unique_lables.size()[0])\n",
        "\n",
        "  for i in unique_lables:\n",
        "    classes_num_partition[i] = torch.sum(class_combs == i)\n",
        "    classes_indices[i.item()] = torch.nonzero(full_train_labels == i).squeeze() #Collects and save Indices in an array based on classe\n",
        "    classes_size[i] = classes_indices[i.item()].size()[0] #Calculate the number of smaples belonging to each class\n",
        "\n",
        "  for i in classes_indices.keys():\n",
        "    classes_indices[i] = classes_indices[i][torch.randperm(classes_indices[i].shape[0])] #suffels the indices\n",
        "\n",
        "  client_indices = {client: torch.tensor([],dtype=torch.int64) for client in range(n_clients) }\n",
        "  assigned_indices = set()\n",
        "\n",
        "  #For each client we generate a element in client_indices dict to keep track of indices we'll associated with each client\n",
        "\n",
        "  for client in range(n_clients):\n",
        "    for cls in class_combs[client]:\n",
        "      cls = cls.item()\n",
        "\n",
        "      portion  = classes_size[cls] /classes_num_partition[cls]\n",
        "      portion = int(portion) if not portion % 1 else int(portion) + 1\n",
        "      portion = min(portion, classes_indices[cls].size()[0])\n",
        "\n",
        "      class_partition = classes_indices[cls][:portion]\n",
        "\n",
        "      class_partition = [idx for idx in class_partition if idx not in assigned_indices]\n",
        "\n",
        "      assigned_indices.update(class_partition)\n",
        "\n",
        "      class_partition = torch.tensor(class_partition, dtype=torch.int64)\n",
        "\n",
        "      client_indices[client] = torch.cat((client_indices[client], class_partition), dim=0)\n",
        "\n",
        "      classes_indices[cls] = classes_indices[cls][portion:]\n",
        "\n",
        "  client_data={\n",
        "      client_id: Subset(dataset.dataset,indices[torch.randperm(len(indices))])\n",
        "      for client_id, indices in client_indices.items()\n",
        "      if len(indices) > 0\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  return client_data, class_combs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_class_combinations(classes, Nc, n_clients):\n",
        "\n",
        "  if Nc * n_clients < len(classes):\n",
        "    Nc = len(classes) / n_clients\n",
        "    Nc = int(Nc) if not Nc % 1 else int(Nc) + 1\n",
        "\n",
        "    print(f\"Number of classes per clients is lower then minimum. Nc changed to {Nc} (the least possible value)\")\n",
        "\n",
        "  combinations = torch.zeros((n_clients,Nc),dtype= torch.int64)\n",
        "  counter =0\n",
        "  ofset = 0\n",
        "  flag = False\n",
        "\n",
        "  for i in range(n_clients):\n",
        "    if not flag:\n",
        "      end_pointer = (i + 1) * Nc\n",
        "      if end_pointer >= classes.size()[0]:\n",
        "          ofset = (end_pointer - classes.size()[0])\n",
        "          flag = True\n",
        "\n",
        "      combinations[i] = classes[i* Nc - ofset: end_pointer - ofset]\n",
        "\n",
        "    else:\n",
        "\n",
        "      combinations[i]  = torch.randperm(classes.size()[0])[:Nc]\n",
        "\n",
        "  return combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqWviCIKs-eN"
      },
      "source": [
        "# Log System\n",
        "\n",
        "In this section Requerd Data will be stored.<br/><br/>\n",
        "**Archaving this information will make it possible to:**\n",
        "*   Handle Clients\n",
        "*   Manage the models\n",
        "*   Keep track of results of different Backbones\n",
        "*   Compare measurement criteria\n",
        "*   Handel model merging process\n",
        "*   Save path to the models\n",
        "\n",
        "<br/><br/>\n",
        "**These data will be saved in two seperted csv file to :**\n",
        "\n",
        "1.   Store the LOCAL Models  \n",
        "2.   Store the GLOBAL Models resulted by each round\n",
        "\n",
        "<br/><br/>\n",
        "The csv files will be handeled as panda.dataframe and each row in the csv file addresses one of models\n",
        "<br/>\n",
        "\n",
        "**Columns (COMMON):**<br/>\n",
        "1. Backbone model name\n",
        "2. Model name\n",
        "3. Path\n",
        "4. Time of log\n",
        "5. Measurement criteria\n",
        " * loss\n",
        " * Accuracy\n",
        " * ...?\n",
        "6. Size of dataset\n",
        "\n",
        "**Columns (Local Models only):**<br/>\n",
        "7. Client Id\n",
        "8. Classes (Indicate which classes have been covered by each client)(format:\"2,4,63,80,9\" or \"all\" for all the classes)\n",
        "9. Round number\n",
        "10. Duration of training\n",
        "11. Train Test ratio\n",
        "\n",
        "**Columns (Global Models only):**<br/>\n",
        "7. Number of clients\n",
        "7. Number of rounds\n",
        "8. Model Aggregation method\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CasHI07Ps4A8"
      },
      "outputs": [],
      "source": [
        "# @title Functions\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import drive\n",
        "from uuid import uuid4\n",
        "import os\n",
        "\n",
        "def get_current_time():\n",
        "  now = datetime.now()\n",
        "\n",
        "  formatted_date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\") # Format the date and time as a string\n",
        "\n",
        "  return formatted_date_time\n",
        "\n",
        "\n",
        "\n",
        "tic_start_time = None\n",
        "\n",
        "def next_id(log_path):\n",
        "  if os.path.exists(log_path):\n",
        "    df = pd.read_csv(log_path)\n",
        "    while True:\n",
        "      uuid = str(uuid4())\n",
        "      if uuid not in df[\"model_name\"].values:\n",
        "        return uuid\n",
        "  else:\n",
        "    return str(uuid4())\n",
        "\n",
        "\n",
        "\n",
        "def tic():\n",
        "    global tic_start_time\n",
        "    tic_start_time = time.perf_counter() # start the timer\n",
        "\n",
        "def toc():\n",
        "    if tic_start_time is None:\n",
        "        print(\"Error: You must call tic() before toc()\")\n",
        "        return None\n",
        "    elapsed_time = time.perf_counter() - tic_start_time\n",
        "    return elapsed_time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l66ikAyqfg4f"
      },
      "source": [
        "# Model and model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiwRCHrwfk63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "\n",
        "class DinoClassifier(nn.Module):\n",
        "  def __init__(self, dino_model, num_classes:int=100, device=None):\n",
        "    super(DinoClassifier, self).__init__()\n",
        "    self.backbone = dino_model\n",
        "\n",
        "    #We need to freaze thhe parameters of bakbone first so we can train only on the head layer(output layer)\n",
        "    for param in self.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    #determine the Device\n",
        "    if device is None:\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    self.backbone.to(device)\n",
        "\n",
        "    #To detect the output feature dimontion of backbone we run  Dummy forward pass\n",
        "    with torch.no_grad():\n",
        "\n",
        "      dummy_input = torch.randn(1,3,224,224).to(device)\n",
        "      dummy_out = self.backbone(dummy_input)\n",
        "\n",
        "\n",
        "      if isinstance(dummy_out, tuple):\n",
        "        dummy_out = dummy_out[0]\n",
        "      elif isinstance(dummy_out, dict):\n",
        "        dummy_out = dummy_out.get(\"x_norm_clstoken\", next(iter(dummy_out.values())))\n",
        "\n",
        "      #If the output is 3D (B, T, D), we assume first token is the [CLS] token.\n",
        "      if dummy_out.dim() == 3:\n",
        "        dummy_feature = dummy_out[:,0]\n",
        "      else:\n",
        "        dummy_feature = dummy_out\n",
        "      feature_dim = dummy_feature.shape[1]\n",
        "      print(\"Detected feature dimontion:\", feature_dim)\n",
        "\n",
        "\n",
        "      #Difineing the classification Head\n",
        "      self.head = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "      #Ensure the head is trainable.\n",
        "      for param in self.head.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    #pass the input through the backbone\n",
        "    features = self.backbone(x)\n",
        "\n",
        "    if isinstance(features, tuple):\n",
        "      features = features[0]\n",
        "    elif isinstance(features, dict):\n",
        "      features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "\n",
        "    # If featers are retuened as (B, T, D), use the first token\n",
        "    if features.dim() == 3:\n",
        "      cls_token = features[:,0]\n",
        "    else:\n",
        "      cls_token = features\n",
        "    logits = self.head(cls_token)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def linear_probe_eval(self, train_set, device, accuracy_threshold=50.0):\n",
        "      \"\"\"Train a linear probe on top of frozen encoder, return per-class accuracy.\"\"\"\n",
        "\n",
        "      self.eval()\n",
        "      # Freeze the backbone (not the head)\n",
        "      for param in self.backbone.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # Linear probe\n",
        "      embedding_dim = self.head.in_features\n",
        "      num_classes = self.head.out_features\n",
        "      linear_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
        "\n",
        "      optimizer = optim.SGD(linear_probe.parameters(), lr=1e-3)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # Probing training\n",
        "      epochs = 5\n",
        "      for epoch in range(epochs):\n",
        "          linear_probe.train()\n",
        "          for x, y in train_set:\n",
        "              x, y = x.to(device), y.to(device)\n",
        "              with torch.no_grad():\n",
        "                  features = self.backbone(x) # Changed from self.encoder to self.backbone\n",
        "                  if isinstance(features, tuple):\n",
        "                    features = features[0]\n",
        "                  elif isinstance(features, dict):\n",
        "                    features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "                  if features.dim() == 3:\n",
        "                    features = features[:, 0]\n",
        "\n",
        "              logits = linear_probe(features)\n",
        "              loss = criterion(logits, y)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "      class_correct = defaultdict(int)\n",
        "      class_total = defaultdict(int)\n",
        "      linear_probe.eval()\n",
        "      with torch.no_grad():\n",
        "          for x, y in train_set:\n",
        "              x, y = x.to(device), y.to(device)\n",
        "              features = self.backbone(x) # Changed from self.encoder to self.backbone\n",
        "              if isinstance(features, tuple):\n",
        "                features = features[0]\n",
        "              elif isinstance(features, dict):\n",
        "                features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "              if features.dim() == 3:\n",
        "                features = features[:, 0]\n",
        "\n",
        "              logits = linear_probe(features)\n",
        "              _, preds = torch.max(logits, 1)\n",
        "              for true, pred in zip(y, preds):\n",
        "                  class_total[int(true)] += 1\n",
        "                  if int(true) == int(pred):\n",
        "                      class_correct[int(true)] += 1\n",
        "\n",
        "      class_accuracy = {\n",
        "          c: 100 * class_correct[c] / class_total[c]\n",
        "          for c in class_total\n",
        "      }\n",
        "\n",
        "      weak_classes = [c for c, acc in class_accuracy.items() if acc < accuracy_threshold]\n",
        "\n",
        "      # print(f\"Client {self.id} - Weak classes detected: {weak_classes}\") # Removed client id here\n",
        "      return class_accuracy, weak_classes\n",
        "\n",
        "  def model_edit(self, weak_classes):\n",
        "    # Placeholder for model editing logic based on weak classes\n",
        "    # This method needs to be implemented based on the specific model editing strategy\n",
        "    print(f\"Editing model based on weak classes: {weak_classes}\") # Added a print statement to show this is called\n",
        "    pass # Replace with actual model editing code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teqJtq_fG47"
      },
      "source": [
        "# Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ynAlV1-oyhY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "import pandas as pd\n",
        "import torch.hub\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader # Import DataLoader\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "class Client:\n",
        "\n",
        "\n",
        "  def __init__(self, id, data, n_clients, spliting_method,num_local_steps = 5, grad_mask = True, sparsity=0.4,lr=1e-3,momentum=0.9, weight_decay= 1e-3, batch_size = 32, classes=\"all\", num_epochs= 10, backbone=None, path_to_model=None, initial_model=None, spliting_ratio={\"train\":0.8, \"test\":0.2}, path_to_subsets=\"\", path_to_class_combs=\"\"):\n",
        "    self.id = id\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Corrected cuda() to is_available()\n",
        "    self.data_set = data\n",
        "    self.spliting_method = spliting_method\n",
        "    self.classes = classes\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.weight_decay = weight_decay\n",
        "    self.backbone = backbone\n",
        "    self.sparsity = sparsity\n",
        "    self.grad_mask = grad_mask\n",
        "    self.path_to_model = path_to_model\n",
        "    self.n_clients = n_clients\n",
        "    if initial_model:\n",
        "      self.model = initial_model\n",
        "    else:\n",
        "      self.model = None\n",
        "    self.load_model()\n",
        "    self.num_epochs = num_epochs\n",
        "    self.spliting_ratio = spliting_ratio\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.train_set , self.test_set = self.test_train_split()\n",
        "    self.num_local_steps = num_local_steps\n",
        "    self.selected_batches = []\n",
        "    self.get_random_batches()\n",
        "    self.duration = 0.0\n",
        "    self.train_loss = None\n",
        "    self.accuracy = None\n",
        "    self.loss = None\n",
        "    self.path_to_subsets = path_to_subsets\n",
        "    self.path_to_class_combs = path_to_class_combs\n",
        "\n",
        "\n",
        "  def test_train_split(self):\n",
        "    train_size = int(self.spliting_ratio.get(\"train\") * len(self.data_set))\n",
        "    test_size =  len(self.data_set) - train_size\n",
        "\n",
        "    train_set, test_set = random_split(self.data_set, [ train_size, test_size ])\n",
        "    train_set = DataLoader(train_set, batch_size=self.batch_size, shuffle=True,  num_workers=2)\n",
        "    test_set = DataLoader(test_set, batch_size=self.batch_size, shuffle=False,  num_workers=2)\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "  def get_random_batches(self):\n",
        "    total_batches = len(self.train_set)\n",
        "    print(total_batches,\"<= total number of batches\")\n",
        "    selected_indices = torch.sort(torch.randperm(total_batches)[:self.num_local_steps])[0]\n",
        "    selected_indices = set(selected_indices.tolist())\n",
        "    self.selected_batches = []\n",
        "\n",
        "    for i, batch in enumerate(self.train_set):\n",
        "      if i in selected_indices:\n",
        "\n",
        "        self.selected_batches.append(batch)\n",
        "\n",
        "\n",
        "  def gradient_mask(self):\n",
        "\n",
        "    masks = {}\n",
        "    all_grads = []\n",
        "\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        all_grads.append((param.grad **2).flatten())\n",
        "\n",
        "    all_grads_flat = torch.cat(all_grads)\n",
        "    k = int(all_grads_flat.numel() * self.sparsity)\n",
        "    threshold = torch.topk(all_grads_flat, k, largest=False).values.max() if k > 0 else 0.0\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        masks[name] = ((param.grad**2) > threshold).float()\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "        if name in masks and param.grad is not None:\n",
        "\n",
        "          param.grad *= masks[name].to(param.grad.device)\n",
        "\n",
        "\n",
        "\n",
        "  # def calculate_fisher_mask(self, n=5):\n",
        "  #   criterion = nn.CrossEntropyLoss()\n",
        "  #   fisher_score = {}\n",
        "  #   last_mask = {}\n",
        "\n",
        "  #   self.model.eval()\n",
        "  #   for param in self.model.head.parameters():\n",
        "  #     param.requires_grad= False\n",
        "\n",
        "  #   for param in self.model.parameters():\n",
        "  #     if param.requires_grad:\n",
        "  #         fisher_score[param] = torch.zeros_like(param.data,device=self.device)\n",
        "  #         last_mask[param] = torch.ones_like(param.data, device=self.device)\n",
        "\n",
        "  #   for i in range(n):\n",
        "  #     for param in fisher_score:\n",
        "  #       fisher_score[param].zero_()\n",
        "\n",
        "  #     for images, labels in self.train_set:\n",
        "  #       images = images.to(self.device)\n",
        "  #       labels = labels.to(self.device)\n",
        "\n",
        "  #       outputs = self.model(images)\n",
        "\n",
        "  #       loss = criterion(outputs, labels)\n",
        "\n",
        "  #       self.model.zero_grad()\n",
        "  #       loss.backward()\n",
        "\n",
        "  #       for param in self.model.parameters():\n",
        "  #           if param.requires_grad and param.grad is not None:\n",
        "  #             fisher_score[param] += (param.grad.data.pow(2) * last_mask[param])\n",
        "\n",
        "  #     new_mask = {}\n",
        "  #     all_scores = torch.cat([torch.flatten(v) for v in fisher_score.values()])\n",
        "  #     non_zero_scores=all_scores[all_scores!=0]\n",
        "  #     k = int(self.sparsity * non_zero_scores.numel())\n",
        "  #     threshold, _ = torch.kthvalue(non_zero_scores, non_zero_scores.numel()-k)\n",
        "\n",
        "  #     for param, score in fisher_score.items():\n",
        "\n",
        "  #         masked_score = score * last_mask[param]\n",
        "  #         current_mask = ((masked_score < threshold) * last_mask[param]).float()\n",
        "  #         new_mask[param] = current_mask\n",
        "  #         last_mask[param] = new_mask[param]\n",
        "  #   self.grad_mask = new_mask\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_fisher_mask(self, n=5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    head_params = [p for p in self.model.head.parameters()]\n",
        "\n",
        "    fisher_scores = {p:torch.zeros_like(p, device=self.device) for p in head_params}\n",
        "    last_mask = {p:torch.ones_like(p, device=self.device) for p in head_params}\n",
        "\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    for _ in range(n):\n",
        "\n",
        "      for v in fisher_scores.values():\n",
        "        v.zero_()\n",
        "\n",
        "      for images, labels in self.train_set:\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        outputs = self.model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        grads = torch.autograd.grad(\n",
        "            loss,\n",
        "            head_params,\n",
        "            create_graph=False,\n",
        "            retain_graph=False\n",
        "        )\n",
        "\n",
        "        for p, g in zip(head_params,grads):\n",
        "          fisher_scores[p] += g.data.pow(2) * last_mask[p]\n",
        "      all_scores = torch.cat([torch.flatten(fisher_scores[p] * last_mask[p])\n",
        "      for p in  head_params])\n",
        "\n",
        "      non_zero  = all_scores[all_scores != 0]\n",
        "\n",
        "      k = int(self.sparsity * non_zero.numel())\n",
        "\n",
        "      threshold, _ = torch.kthvalue(non_zero, non_zero.numel() - k + 1)\n",
        "\n",
        "      new_mask = {}\n",
        "\n",
        "      for p in head_params:\n",
        "        masked_scores = fisher_scores[p] * last_mask[p]\n",
        "        current_mask = (masked_scores >= threshold).float() * last_mask[p]\n",
        "        new_mask[p]  = current_mask\n",
        "        last_mask[p] = current_mask\n",
        "\n",
        "\n",
        "    self.grad_mask = new_mask\n",
        "\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "\n",
        "    if self.path_to_model:\n",
        "      dino_model = torch.hub.load('facebookresearch/dino:main', self.backbone)\n",
        "      self.model = DinoClassifier(dino_model=dino_model, num_classes=100, device=self.device)\n",
        "      state_dict = torch.load(self.path_to_model)\n",
        "      self.model.load_state_dict(state_dict)\n",
        "\n",
        "    self.model.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train_default(self):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(self.model.head.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "    tic()\n",
        "    self.calculate_fisher_mask()\n",
        "    # self.calculate_fisher_mask()\n",
        "    for step_num in range(len(self.selected_batches)):\n",
        "      self.model.train()\n",
        "      for param in self.model.head.parameters():\n",
        "        param.requires_grad = True\n",
        "      running_loss = 0.0\n",
        "\n",
        "      # for (images, labels) in enumerate(self.selected_batches[step_num]):\n",
        "      images, labels = self.selected_batches[step_num]\n",
        "      images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = self.model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      if self.grad_mask:\n",
        "        self.gradient_mask()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # loc_stp_loss = running_loss / len(self.train_set)\n",
        "      avg_loss = running_loss / len(self.selected_batches)\n",
        "      print(f\"client {self.id}- local step number {step_num} - step loss:{avg_loss:.4f} - batch number: {step_num}\" )\n",
        "      self.duration = toc()\n",
        "      self.train_loss = avg_loss\n",
        "\n",
        "  def SGDM(self, params, damping, nesterov,max_ , buffer):\n",
        "    for param in params:\n",
        "      if param.grad is None:\n",
        "        continue\n",
        "      grad = param.grad.data\n",
        "\n",
        "      if self.weight_decay != 0:\n",
        "        grad = grad.add(param.data, alpha=self.weight_decay)\n",
        "\n",
        "      buf = buffer.get(param,None)\n",
        "      if buf is None:\n",
        "        buf[param] = torch.zeros_like(param.data)\n",
        "\n",
        "      if self.momentum != 0 :\n",
        "        buf_new = buf.mul(self.momentum).add(grad, alpha=(1 - damping))\n",
        "        update = grad.add(buf_new, alpha= self.momentum) if nesterov else buf_new\n",
        "\n",
        "\n",
        "\n",
        "      else:\n",
        "        buf_new = torch.zeros_like(param.data)\n",
        "        update = grad\n",
        "\n",
        "\n",
        "      update = update * self.grad_mask\n",
        "\n",
        "      if max_:\n",
        "        param.data.add_(update, alpha=self.lr)\n",
        "\n",
        "      else:\n",
        "        param.data.add_(update, alpha=-self.lr)\n",
        "\n",
        "\n",
        "      buffer[param] = buf_new\n",
        "    return buffer\n",
        "\n",
        "\n",
        "\n",
        "  def SGDM_train(self):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    tic()\n",
        "    self.calculate_fisher_mask()\n",
        "    # self.calculate_fisher_mask()\n",
        "\n",
        "    damping = 0\n",
        "    nesterov= False\n",
        "    max_ = False\n",
        "\n",
        "    buffer = {}\n",
        "    running_loss = 0.0\n",
        "\n",
        "    self.model.train()\n",
        "    for param in self.model.head.parameters():\n",
        "      param.requires_grad = True\n",
        "    head_params = [p for p in self.model.head.parameters() if p.requires_grad]\n",
        "\n",
        "    for step_num in range(len(self.selected_batches)):\n",
        "      images, labels = self.selected_batches[step_num]\n",
        "      images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "\n",
        "      outputs = self.model(images)\n",
        "      self.model.zero_grad()\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      buffer = self.SGDM( params= head_params,\n",
        "                    damping=damping,\n",
        "                    nesterov=nesterov,\n",
        "                    max_=max_ ,\n",
        "                    buffer=buffer )\n",
        "\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # loc_stp_loss = running_loss / len(self.train_set)\n",
        "      avg_loss = running_loss / len(self.selected_batches)\n",
        "      print(f\"client {self.id}- local step number {step_num} - step loss:{avg_loss:.4f} - batch number: {step_num}\" )\n",
        "    self.duration = toc()\n",
        "    self.train_loss = avg_loss\n",
        "\n",
        "\n",
        "  def evaluate(self):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    self.model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for index, (images, labels) in enumerate(self.train_set):\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        outputs = self.model(images)\n",
        "\n",
        "        _, prediction = torch.max(outputs.data,1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * labels.size(0)\n",
        "\n",
        "\n",
        "        total += labels.size(0)\n",
        "\n",
        "        correct += (prediction == labels).sum().item()\n",
        "    self.accuracy = 100 * correct / total\n",
        "    self.loss = test_loss / total\n",
        "\n",
        "\n",
        "  def confirm_save(self,path):\n",
        "      torch.save(self.model.head.state_dict(),  path )\n",
        "\n",
        "  def create_log(self, model_name, path, round_number):\n",
        "\n",
        "    log_dict= {\n",
        "        \"client_id\":[self.id],\n",
        "        \"backbone\":[self.backbone],\n",
        "        \"model_name\":[model_name],\n",
        "        \"initial_model_name\":[initial_model_name],\n",
        "        \"path\": [path],\n",
        "        \"num_of_clients\":[self.n_clients],\n",
        "        \"Measurement_criteria\":[\"accuracy,loss,train_loss\"],\n",
        "        \"accuracy\":[self.accuracy],\n",
        "        \"loss\":[self.loss],\n",
        "        \"train_loss\":[self.train_loss],\n",
        "        \"splitting_method\":[self.spliting_method],\n",
        "        \"gradient_mask\":[self.grad_mask],\n",
        "        \"sparsity\":[self.sparsity],\n",
        "        \"size_of_dataset\": [len(self.data_set.dataset)],\n",
        "        \"client_train_size\":[len(self.train_set.dataset)],\n",
        "        \"client_test_size\":[len(self.test_set.dataset)],\n",
        "        \"train_test_ratio\":[self.spliting_ratio],\n",
        "        \"classes\":[self.classes],\n",
        "        \"round_number\":[round_number],\n",
        "        \"duration\":[self.duration],\n",
        "        \"time\": [get_current_time()],\n",
        "        \"path_to_subsets\":[self.path_to_subsets],\n",
        "        \"path_to_class_combs\":[self.path_to_class_combs]\n",
        "    }\n",
        "\n",
        "\n",
        "    return pd.DataFrame(log_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>❗ Important Notice ❗</h1>**\n",
        "\n",
        "**Regarding `save_data`:**\n",
        "Please be aware that checking the `save_data` option will generate a **new data subset** and a **new initial model** based on your specified parameters.\n",
        "\n",
        "**⚠️ Crucial: Using Existing Models with New Data/Parameters ⚠️**\n",
        "If you intend to use an *existing model* but wish to apply it to a *different data subset*, use a *different data splitting method*, or make *any other changes to the data or algorithm*, you **MUST** assign a **new and unique model name**.\n",
        "\n",
        "**Why is this critical?**\n",
        "Failing to use a unique model name will make it impossible to differentiate between models for each client when filtering. This will lead to inaccurate results from the client aggregation function on the server."
      ],
      "metadata": {
        "id": "aw0_IYcv1m7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_data = False #@param{\"type\":\"boolean\"}\n",
        "initial_model_name = \"a24812b6-0399-4992-ba2e-1e5747ff2536\" #@param{\"type\":\"string\"}\n",
        "path_to_subsets = \"/content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/client_data_iid_20clients.pth\" # @param {\"type\":\"string\"}\n",
        "path_to_class_combs = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if load_data:\n",
        "\n",
        "  initial_model_log_df = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "  initial_model_path = initial_model_log_df[initial_model_log_df[\"model_name\"] == initial_model_name][\"path\"].values[0]\n",
        "  initial_model_round_num = initial_model_log_df[initial_model_log_df[\"model_name\"] == initial_model_name][\"round_number\"].values[0]\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  dino_model = torch.hub.load('facebookresearch/dino:main', backbone)\n",
        "  initial_model = DinoClassifire(dino_model=dino_model, num_classes=100, device=device)\n",
        "  initial_model.load_state_dict(torch.load(initial_model_path))\n",
        "\n",
        "  client_data = torch.load(path_to_subsets, weights_only=False)\n",
        "  if not spliting_method == \"i.i.d. sharing\":\n",
        "    class_combs = torch.load(path_to_class_combs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "56FLr8HaszZz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extraction of the model we want to use as initial model\n",
        "if load_data:\n",
        "  global_log = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "\n",
        "\n",
        "  filter = (global_log[\"aggregation_method\"] == \"EMA\") & (global_log[\"round_number\"]==2)\n",
        "  filtered_models = global_log[filter]\n",
        "\n",
        "  print(filtered_models[\"model_name\"].values)\n",
        "\n",
        "  filtered_models.head()\n"
      ],
      "metadata": {
        "id": "x1fIsj0tMGjo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "save_data = True #@param{\"type\":\"boolean\"}\n",
        "\n",
        "if load_data:\n",
        "  save_data = False\n",
        "\n",
        "if save_data:\n",
        "  method = \"iid\" if spliting_method == \"i.i.d. sharing\" else \"noniid\"\n",
        "  if spliting_method == \"i.i.d. sharing\":\n",
        "    path_to_subsets = f\"/content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/client_data_{method}_{str(n_clients)}clients_{str(uuid4())}.pth\"\n",
        "    client_data = iid_sharing(train_set, n_clients)\n",
        "    class_combs = \"all\"\n",
        "    print(spliting_method)\n",
        "  else:\n",
        "    client_data, class_combs = noniid_sharing(train_set,Nc=Nc, n_clients=n_clients)\n",
        "    path_to_class_combs = f\"/content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/class_combs_{method}_{str(n_clients)}clients_{str(uuid4())}.pth\"\n",
        "    print(spliting_method)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  dino_model = torch.hub.load('facebookresearch/dino:main', backbone)\n",
        "  initial_model = DinoClassifier(dino_model=dino_model, num_classes=100, device=device)\n",
        "  initial_model_name = next_id(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "  initial_model_path = \"/content/drive/MyDrive/MLDL_FederatedLearning/models/global/\" + initial_model_name + \".pth\"\n",
        "  initial_model_round_num = 0\n",
        "  initial_model_log = {\n",
        "    \"backbone\": [backbone],\n",
        "    \"model_name\": [initial_model_name],\n",
        "    \"num_of_clients\": [n_clients],\n",
        "    \"path\": [initial_model_path],\n",
        "    \"Measurement_criteria\": [None],\n",
        "    \"prev_global_model_name\":[None],\n",
        "    \"accuracy\": [None],\n",
        "    \"loss\": [None],\n",
        "    \"splitting_method\": [spliting_method],\n",
        "    \"size_of_dataset\": [len(train_dataset)],\n",
        "    \"train_test_ratio\": [None],\n",
        "    \"classes\": [None],\n",
        "    \"round_number\": [0],\n",
        "    \"time\": [get_current_time()],\n",
        "    \"path_to_subsets\": [path_to_subsets],\n",
        "    \"path_to_class_combs\": [path_to_class_combs],\n",
        "    \"num_of_participants\": [None]\n",
        "}\n",
        "  initial_model_log[\"aggregation_method\"] =[ np.nan]\n",
        "  initial_model_log[\"contributors\"] =[ np.nan]\n",
        "  initial_model_log[\"momentum_vector_path\"] = [np.nan]\n",
        "\n",
        "\n",
        "  initial_model_log = pd.DataFrame(initial_model_log)\n",
        "  initial_model_log = initial_model_log[['backbone',\n",
        "                'num_of_clients',\n",
        "                'splitting_method',\n",
        "                'aggregation_method',\n",
        "                'Measurement_criteria',\n",
        "                'accuracy',\n",
        "                'loss',\n",
        "                'size_of_dataset',\n",
        "                'train_test_ratio',\n",
        "                'classes',\n",
        "                'round_number',\n",
        "                'num_of_participants',\n",
        "                'model_name',\n",
        "                'prev_global_model_name',\n",
        "                \"contributors\",\n",
        "                'path',\n",
        "                \"momentum_vector_path\",\n",
        "                'path_to_subsets',\n",
        "                'path_to_class_combs',\n",
        "                'time'\n",
        "                ]]\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\"):\n",
        "    initial_model_log.to_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\", index=False)\n",
        "  else:\n",
        "    initial_model_log.to_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\", mode='a', header=False, index=False)\n",
        "\n",
        "  torch.save(initial_model.state_dict(), initial_model_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  torch.save(client_data, path_to_subsets)\n",
        "  if method== \"noniid\":\n",
        "    torch.save(class_combs, path_to_class_combs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FSnlxpSszYyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ed8bb5-88c6-42ee-d347-33ad4b17c51b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i.i.d. sharing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n",
            "100%|██████████| 82.7M/82.7M [00:00<00:00, 380MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected feature dimontion: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random clients selection\n",
        "\n",
        "global_log = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "\n",
        "\n",
        "filter = (global_log[\"model_name\"] == initial_model_name)\n",
        "filtered_models = global_log[filter]\n",
        "\n",
        "r_num = filtered_models[\"round_number\"].values[0]\n",
        "\n",
        "selection_percentage = 10 #@param {\"type\":\"integer\"}\n",
        "set_seed(int(r_num),is_seed_fixed)\n",
        "def get_random_clients(n_clients, initial_model_name, selection_percentage=10):\n",
        "  if os.path.exists(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\"):\n",
        "    clients_df = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\")\n",
        "    clients_df = clients_df[clients_df['initial_model_name']== initial_model_name]\n",
        "    selected_clients = clients_df['client_id'].values\n",
        "  else:\n",
        "    selected_clients = np.array([], dtype=np.int16)\n",
        "  while len(selected_clients) < (selection_percentage / 100 ) * n_clients:\n",
        "    rand_int = torch.randint(0,n_clients,(1,))[0].item()\n",
        "    if rand_int not in selected_clients:\n",
        "      selected_clients = np.append(selected_clients,rand_int)\n",
        "\n",
        "  return selected_clients\n",
        "\n",
        "selected_clients = get_random_clients(n_clients,initial_model_name,selection_percentage)\n",
        "print(selected_clients)"
      ],
      "metadata": {
        "id": "T6FBeYb1bWNp",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ecd280-4904-482b-fa21-265357bcd2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44 39 33 60 63 79 27  3 97 83]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcRGW2lVdE8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "b98b20c5-18cd-4d60-c9a4-a75fa51afa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################################################################################\n",
            "12 <= total number of batches\n",
            "Data size:  449\n",
            "Backbone:  dino_vits16\n",
            "client 44- local step number 0 - step loss:1.7484 - batch number: 0\n",
            "client 44- local step number 1 - step loss:1.5136 - batch number: 1\n",
            "client 44- local step number 2 - step loss:1.5006 - batch number: 2\n",
            "client 44- local step number 3 - step loss:1.3926 - batch number: 3\n",
            "client 44- local step number 4 - step loss:1.3568 - batch number: 4\n",
            "####################################################################################################\n",
            "12 <= total number of batches\n",
            "Data size:  449\n",
            "Backbone:  dino_vits16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-250052480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# Use the save_client method from the Client class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3674512377.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mtic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_fisher_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;31m# self.calculate_fisher_mask()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3674512377.py\u001b[0m in \u001b[0;36mcalculate_fisher_mask\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "\n",
        "if not spliting_method == \"i.i.d. sharing\":\n",
        "  path_to_class_combs = \" \"\n",
        "\n",
        "log_file = \"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\"\n",
        "\n",
        "\n",
        "for client_num in selected_clients:\n",
        "      all_clients_df = pd.read_csv(log_file)\n",
        "      filtered_clients_df = all_clients_df[all_clients_df[\"initial_model_name\"] == initial_model_name]\n",
        "      print(\"#\"*100)\n",
        "      if client_num in filtered_clients_df[\"client_id\"].values:\n",
        "        print(f\"Client {client_num} is already trained\")\n",
        "        continue\n",
        "    # try:\n",
        "      client = Client(id=client_num,\n",
        "                    data=client_data[client_num] ,\n",
        "                    spliting_method=spliting_method,\n",
        "                    classes=\"all\",\n",
        "                    n_clients=n_clients,\n",
        "                    batch_size = batch_size,\n",
        "                    num_epochs= 10,\n",
        "                    grad_mask= True,\n",
        "                    initial_model = copy.deepcopy(initial_model),\n",
        "                    backbone=backbone,\n",
        "                    path_to_model=None,\n",
        "                    spliting_ratio={\"train\":0.8, \"test\":0.2},\n",
        "                    path_to_subsets=path_to_subsets,\n",
        "                    path_to_class_combs=path_to_class_combs\n",
        "                    )\n",
        "      print(\"Data size: \",len(client_data[client_num]))\n",
        "      print(\"Backbone: \", backbone)\n",
        "\n",
        "\n",
        "      client.train()\n",
        "      client.evaluate()\n",
        "      # Use the save_client method from the Client class\n",
        "      log = client.create_log(\n",
        "          model_name=next_id(log_file), # Generate a new model name\n",
        "          path=f\"/content/drive/MyDrive/MLDL_FederatedLearning/models/clients/{next_id(log_file)}.pth\", # Generate a new path\n",
        "          round_number=initial_model_round_num + 1\n",
        "          )\n",
        "      # client.confirm_save(log['path'][0]) # Save the model\n",
        "\n",
        "      # if not os.path.exists(log_file):\n",
        "      #   log.to_csv(log_file, index=False)\n",
        "\n",
        "      #   print(\"new csv file \")\n",
        "      #   print(f\"name: {log['model_name'][0]} \")\n",
        "      #   print(f\"path: {log['path'][0]} \")\n",
        "      #   print(f\"Logged client {client_num} to {log_file}\")\n",
        "\n",
        "      # else: # HERE\n",
        "      # # Create a new CSV file IF path doesn't exist\n",
        "      #   # This check is no longer necessary as we generate a new id and path every time\n",
        "      #   # path_check = pd.read_csv(log_file)['model_name'].values # model_name\n",
        "\n",
        "      #   # if log['model_name'][0] not in path_check:\n",
        "      #   client.confirm_save(log['path'][0])\n",
        "      #   log.to_csv(log_file, mode='a', header=False, index=False)\n",
        "      #   print(f\"name: {log['model_name'][0]} \")\n",
        "      #   print(f\"path: {log['path'][0]} \")\n",
        "      #   print(f\"Logged client {client_num} to {log_file}\")\n",
        "      #   # else :\n",
        "      #   #   print(\"Existing Log\")\n",
        "      del client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv')  # Change to your file path and format\n",
        "\n",
        "# Drop rows where 'path' is NaN or not a string\n",
        "df = df[df['path'].notna()]\n",
        "df['path'] = df['path'].astype(str).str.strip()  # Remove whitespace\n",
        "\n",
        "# Check if each path exists\n",
        "df_filtered = df[df['path'].apply(lambda x: os.path.exists(x))]\n",
        "n_error = len(df[\"client_id\"].values) -  len(df_filtered[\"client_id\"].values)\n",
        "print(f\" {n_error} clients' log are removed due to the not existing saved model file\")\n",
        "# Save the cleaned DataFrame\n",
        "df_filtered.to_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv', index=False)\n",
        "# print(len(df),len(df_filtered))"
      ],
      "metadata": {
        "id": "_FtdurVygZ__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29a55f1-37c9-46ed-b3e2-e76303d5ed74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 clients' log are removed due to the not existing saved model file\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fu0Iz0LDlRDK",
        "M9hv0ik3jZ-8",
        "t4oXrqMHwAit",
        "bVtH-qLIrAeh",
        "l66ikAyqfg4f"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}